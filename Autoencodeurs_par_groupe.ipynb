{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelleRakotovao/Algo-ML/blob/main/Autoencodeurs_par_groupe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RSk9oJglZfBg",
      "metadata": {
        "id": "RSk9oJglZfBg"
      },
      "source": [
        "# Instructions :\n",
        "Travail en groupe, à  placer dans le premier dépot Git de notre année Universitaire, dans le dossier <code>Computer Vision</code>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FtFerpnCZ_2d",
      "metadata": {
        "id": "FtFerpnCZ_2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "-gh-tuS0XSAZ",
      "metadata": {
        "id": "-gh-tuS0XSAZ"
      },
      "source": [
        "# Membres du Groupe :\n",
        "\n",
        "1.   RAHERIMANANA Andriniaina Koloina Mandresy ( n° = 05)\n",
        "2.   RAKOTOVAO Holiantenaina Josée Michelle    ( n° = 17)\n",
        "3.   TOVO Jean Bien Aimé                       ( n° = 15)\n",
        "4.   RAJOHARIVELO Andriarivony Antenaina       ( n° = 01)\n",
        "5.   RAKOTOMAHARAVO Fanomezantsoa Vali         ( n° = 12)\n",
        "6.\n",
        "7.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z-w6UuFuXRxC",
      "metadata": {
        "id": "Z-w6UuFuXRxC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ovQZFqUv58vy",
      "metadata": {
        "id": "ovQZFqUv58vy"
      },
      "source": [
        "# Autoencodeurs pour Images de Cercles\n",
        "\n",
        "Dans ce notebook, nous allons générer un dataset synthétique où chaque image est une image en niveaux de gris de taille 64x64 contenant un cercle blanc (de centre et rayon aléatoires) sur fond noir. Le but est d'entraîner des autoencodeurs qui réduisent ces images dans un espace latent de dimension 3, puis les reconstruisent.\n",
        "\n",
        "Nous étudierons deux configurations :\n",
        "- Un autoencodeur entièrement connecté\n",
        "- Un autoencodeur convolutionnel\n",
        "\n",
        "Vous pourrez comparer leurs performances et discuter des compromis liés à la conception de l'espace latent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faIvPWij58vy",
      "metadata": {
        "id": "faIvPWij58vy"
      },
      "source": [
        "## Étape 1 : Génération du Dataset\n",
        "\n",
        "Nous allons générer un dataset d'images où chaque image comporte un cercle blanc de centre et de rayon aléatoires. Pour cela, nous utiliserons la bibliothèque OpenCV pour dessiner le cercle sur une image noire.\n",
        "\n",
        "### Explication Pratique\n",
        "La génération d'un dataset synthétique permet de contrôler précisément les paramètres (ici, le centre et le rayon du cercle) et de disposer d'une base simple pour expérimenter la réduction dimensionnelle via l'espace latent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1x2h3deq58vz",
      "metadata": {
        "id": "1x2h3deq58vz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def generate_circle_image(image_size=64, min_radius=5, max_radius=20):\n",
        "    \"\"\"Génère une image en niveaux de gris de taille image_size x image_size contenant un cercle blanc.\"\"\"\n",
        "    # Créer une image noire\n",
        "    image = np.zeros((image_size, image_size), dtype=np.uint8)\n",
        "\n",
        "    # Choisir un rayon aléatoire\n",
        "    radius = np.random.randint(min_radius, max_radius)\n",
        "\n",
        "    # Choisir un centre aléatoire de sorte que le cercle soit entièrement contenu dans l'image\n",
        "    x = np.random.randint(radius, image_size - radius)\n",
        "    y = np.random.randint(radius, image_size - radius)\n",
        "\n",
        "    # Dessiner le cercle (couleur blanche: 255, épaisseur=-1 pour un remplissage complet)\n",
        "    cv2.circle(image, (x, y), radius, 255, -1)\n",
        "\n",
        "    # Normaliser l'image entre 0 et 1\n",
        "    image = image.astype('float32') / 255.0\n",
        "\n",
        "    return image\n",
        "\n",
        "def generate_dataset(n_samples=10000, image_size=64):\n",
        "    data = []\n",
        "    for _ in range(n_samples):\n",
        "        img = generate_circle_image(image_size=image_size)\n",
        "        data.append(img)\n",
        "    data = np.array(data)\n",
        "    # Ajouter une dimension pour le canal (images en niveaux de gris)\n",
        "    data = np.expand_dims(data, axis=-1)\n",
        "    return data\n",
        "\n",
        "# Générer le dataset\n",
        "data = generate_dataset(n_samples=10000, image_size=64)\n",
        "\n",
        "# Afficher quelques exemples\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(data[i].squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B3tnYLzz58vz",
      "metadata": {
        "id": "B3tnYLzz58vz"
      },
      "source": [
        "## Étape 2 : Préparation des Données\n",
        "\n",
        "Nous allons diviser le dataset en ensembles d'entraînement et de test. Ici, nous utilisons 80 % des données pour l'entraînement et 20 % pour le test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L6C0j6i658vz",
      "metadata": {
        "id": "L6C0j6i658vz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print('Entraînement:', train_data.shape)\n",
        "print('Test:', test_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0Aub7958vz",
      "metadata": {
        "id": "0a0Aub7958vz"
      },
      "source": [
        "## Étape 3 : Autoencodeur Entièrement Connecté (Dense Layers)\n",
        "\n",
        "Dans cette partie, nous construisons un autoencodeur entièrement connecté. Le modèle comporte :\n",
        "\n",
        "- **Encodeur :** Un aplatissement de l'image suivi d'une couche Dense de 128 neurones (activation ReLU) puis une couche Dense de 3 neurones qui représente l'espace latent.\n",
        "- **Décodeur :** Une couche Dense de 128 neurones (activation ReLU), suivie d'une couche Dense de 4096 neurones (pour reconstruire une image 64x64) avec activation sigmoïde, puis un reshape en (64,64,1).\n",
        "\n",
        "### Explication Pratique\n",
        "Cet autoencodeur simple permet de voir comment les informations (ici, la position et la taille du cercle) sont comprimées dans un espace de très faible dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9WPH5zyl58vz",
      "metadata": {
        "id": "9WPH5zyl58vz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_img = layers.Input(shape=(64, 64, 1))\n",
        "x = layers.Flatten()(input_img)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "latent = layers.Dense(3, name='latent_space')(x)  # Espace latent de dimension 3\n",
        "\n",
        "x = layers.Dense(128, activation='relu')(latent)\n",
        "x = layers.Dense(64*64, activation='sigmoid')(x)\n",
        "decoded = layers.Reshape((64, 64, 1))(x)\n",
        "\n",
        "autoencoder_fc = models.Model(input_img, decoded)\n",
        "\n",
        "autoencoder_fc.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder_fc.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G82wkO3158v0",
      "metadata": {
        "id": "G82wkO3158v0"
      },
      "source": [
        "### Entraînement de l'Autoencodeur Entièrement Connecté FCN (Dense layers)\n",
        "\n",
        "Nous allons entraîner cet autoencodeur sur l'ensemble d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aKT1pLpF58v0",
      "metadata": {
        "id": "aKT1pLpF58v0"
      },
      "outputs": [],
      "source": [
        "history_fc = autoencoder_fc.fit(train_data, train_data,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=128,\n",
        "                                  shuffle=True,\n",
        "                                  validation_data=(test_data, test_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QjSp5D-e58v0",
      "metadata": {
        "id": "QjSp5D-e58v0"
      },
      "source": [
        "### Visualisation des Reconstructions (FC)\n",
        "\n",
        "Affichons quelques reconstructions pour évaluer qualitativement les performances de l'autoencodeur entièrement connecté."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SwVLV4M958v0",
      "metadata": {
        "id": "SwVLV4M958v0"
      },
      "outputs": [],
      "source": [
        "decoded_imgs_fc = autoencoder_fc.predict(test_data)\n",
        "\n",
        "n = 8\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Afficher l'image originale\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(test_data[i].squeeze(), cmap='gray')\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Afficher l'image reconstruite\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs_fc[i].squeeze(), cmap='gray')\n",
        "    plt.title('Reconstruit')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v6mJtIMW58v0",
      "metadata": {
        "id": "v6mJtIMW58v0"
      },
      "source": [
        "## Étape 4 : Autoencodeur Convolutionnel\n",
        "\n",
        "Nous allons maintenant construire un autoencodeur convolutionnel. Cette architecture est généralement mieux adaptée pour traiter des images car elle exploite la structure spatiale.\n",
        "\n",
        "### Architecture\n",
        "- **Encodeur :**\n",
        "  - Conv2D(32, 3, activation='relu', padding='same')\n",
        "  - MaxPooling2D((2,2), padding='same')\n",
        "  - Conv2D(64, 3, activation='relu', padding='same')\n",
        "  - MaxPooling2D((2,2), padding='same')\n",
        "  - Flatten puis Dense(3) pour obtenir le latent space\n",
        "- **Décodeur :**\n",
        "  - Dense(16*16*64, activation='relu') puis Reshape en (16,16,64)\n",
        "  - Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')\n",
        "  - Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')\n",
        "  - Conv2D(1, 3, activation='sigmoid', padding='same') pour reconstruire l'image\n",
        "\n",
        "### Explication Pratique\n",
        "Les autoencodeurs convolutionnels capturent mieux la structure locale des images, ce qui peut améliorer la qualité des reconstructions par rapport à une architecture entièrement connectée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ucCIxsih58v0",
      "metadata": {
        "id": "ucCIxsih58v0"
      },
      "outputs": [],
      "source": [
        "input_img = layers.Input(shape=(64, 64, 1))\n",
        "\n",
        "# Encodeur\n",
        "x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
        "x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
        "x = layers.Flatten()(x)\n",
        "latent = layers.Dense(3, name='latent_space')(x)\n",
        "\n",
        "# Décodeur\n",
        "x = layers.Dense(16*16*64, activation='relu')(latent)\n",
        "x = layers.Reshape((16,16,64))(x)\n",
        "x = layers.Conv2DTranspose(64, (3,3), strides=2, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2DTranspose(32, (3,3), strides=2, activation='relu', padding='same')(x)\n",
        "decoded = layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder_conv = models.Model(input_img, decoded)\n",
        "\n",
        "autoencoder_conv.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder_conv.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tKC-mnea58v0",
      "metadata": {
        "id": "tKC-mnea58v0"
      },
      "source": [
        "### Entraînement de l'Autoencodeur Convolutionnel\n",
        "\n",
        "Entraînons cet autoencodeur sur le même ensemble d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7XfU38r58v0",
      "metadata": {
        "id": "l7XfU38r58v0"
      },
      "outputs": [],
      "source": [
        "history_conv = autoencoder_conv.fit(train_data, train_data,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=128,\n",
        "                                  shuffle=True,\n",
        "                                  validation_data=(test_data, test_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V6bwbUqO58v0",
      "metadata": {
        "id": "V6bwbUqO58v0"
      },
      "source": [
        "### Visualisation des Reconstructions (Convolutionnel)\n",
        "\n",
        "Affichons quelques reconstructions pour comparer qualitativement les deux architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e_9ZbPgr58v0",
      "metadata": {
        "id": "e_9ZbPgr58v0"
      },
      "outputs": [],
      "source": [
        "decoded_imgs_conv = autoencoder_conv.predict(test_data)\n",
        "\n",
        "n = 8\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Afficher l'image originale\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(test_data[i].squeeze(), cmap='gray')\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Afficher l'image reconstruite\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs_conv[i].squeeze(), cmap='gray')\n",
        "    plt.title('Reconstruit')\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brZ5QizY58v0",
      "metadata": {
        "id": "brZ5QizY58v0"
      },
      "source": [
        "## Expérimentations\n",
        "\n",
        "1. Tester les deux modèles sur des images d'entrées imparfaites (avec les imperfections de votre choix : parties cachées, bruits, ...). Interpréter les résultats.\n",
        "2. Améliorer un des modèles ci-dessus de votre choix et montrer le résultat (sans modifier la taille de l'espace latent).\n",
        "3. Selon les expériences menées dans ce notebook, votre avis et vos conaissances, est-il nécessaire d'augmenter la taille de l'espace latent pour avoir un réseau à faire ce travail. **Argumenter** votre réponse."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}